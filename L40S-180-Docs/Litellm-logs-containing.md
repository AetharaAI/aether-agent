r-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: spend_update_queue.py:36 - Adding update to queue: {'entity_type': <Litellm_EntityType.KEY: 'key'>, 'entity_id': '054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d', 'response_cost': 0.0}
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: db_spend_update_writer.py:292 - track_cost_callback: team_id is None or prisma_client is None. Not tracking spend for team
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: db_spend_update_writer.py:333 - track_cost_callback: org_id is None or prisma_client is None. Not tracking spend for org
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: spend_update_queue.py:36 - Adding update to queue: {'entity_type': <Litellm_EntityType.TAG: 'tag'>, 'entity_id': 'User-Agent: curl', 'response_cost': 0.0}
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: spend_update_queue.py:36 - Adding update to queue: {'entity_type': <Litellm_EntityType.TAG: 'tag'>, 'entity_id': 'User-Agent: curl/7.81.0', 'response_cost': 0.0}
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: db_spend_update_writer.py:1331 - Logged request status: success
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: daily_spend_update_queue.py:62 - Adding update to queue: {'default_user_id_2026-02-05_054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d_kimi-vl-thinking_openai': {'user_id': 'default_user_id', 'date': '2026-02-05', 'api_key': '054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d', 'model': 'kimi-vl-thinking', 'model_group': 'kimi-vl-thinking', 'mcp_namespaced_tool_name': None, 'custom_llm_provider': 'openai', 'prompt_tokens': 18, 'completion_tokens': 256, 'spend': 0.0, 'api_requests': 1, 'successful_requests': 1, 'failed_requests': 0, 'cache_read_input_tokens': 0, 'cache_creation_input_tokens': 0}}
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: db_spend_update_writer.py:1331 - Logged request status: success
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: daily_spend_update_queue.py:62 - Adding update to queue: {'_2026-02-05_054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d_kimi-vl-thinking_openai': {'team_id': '', 'date': '2026-02-05', 'api_key': '054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d', 'model': 'kimi-vl-thinking', 'model_group': 'kimi-vl-thinking', 'mcp_namespaced_tool_name': None, 'custom_llm_provider': 'openai', 'prompt_tokens': 18, 'completion_tokens': 256, 'spend': 0.0, 'api_requests': 1, 'successful_requests': 1, 'failed_requests': 0, 'cache_read_input_tokens': 0, 'cache_creation_input_tokens': 0}}
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: db_spend_update_writer.py:1448 - organization_id is None for request. Skipping incrementing organization spend.
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: db_spend_update_writer.py:1331 - Logged request status: success
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: daily_spend_update_queue.py:62 - Adding update to queue: {'User-Agent: curl_2026-02-05_054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d_kimi-vl-thinking_openai': {'tag': 'User-Agent: curl', 'date': '2026-02-05', 'api_key': '054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d', 'model': 'kimi-vl-thinking', 'model_group': 'kimi-vl-thinking', 'mcp_namespaced_tool_name': None, 'custom_llm_provider': 'openai', 'prompt_tokens': 18, 'completion_tokens': 256, 'spend': 0.0, 'api_requests': 1, 'successful_requests': 1, 'failed_requests': 0, 'cache_read_input_tokens': 0, 'cache_creation_input_tokens': 0, 'request_id': 'chatcmpl-8a58442c37bbd59f_cache_hit1770297794.5977585'}}
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: daily_spend_update_queue.py:62 - Adding update to queue: {'User-Agent: curl/7.81.0_2026-02-05_054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d_kimi-vl-thinking_openai': {'tag': 'User-Agent: curl/7.81.0', 'date': '2026-02-05', 'api_key': '054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d', 'model': 'kimi-vl-thinking', 'model_group': 'kimi-vl-thinking', 'mcp_namespaced_tool_name': None, 'custom_llm_provider': 'openai', 'prompt_tokens': 18, 'completion_tokens': 256, 'spend': 0.0, 'api_requests': 1, 'successful_requests': 1, 'failed_requests': 0, 'cache_read_input_tokens': 0, 'cache_creation_input_tokens': 0, 'request_id': 'chatcmpl-8a58442c37bbd59f_cache_hit1770297794.5977585'}}
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: proxy_server.py:1275 - _update_key_cache: hashed_token=054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: proxy_server.py:1277 - _update_key_cache: existing_spend_obj=token='054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d' key_name=None key_alias=None spend=0.0 max_budget=None expires=None models=[] aliases={} config={} user_id='default_user_id' team_id=None max_parallel_requests=None metadata={} tpm_limit=None rpm_limit=None budget_duration=None budget_reset_at=None allowed_cache_controls=[] allowed_routes=[] permissions={} model_spend={} model_max_budget={} soft_budget_cooldown=False blocked=None litellm_budget_table=None org_id=None created_at=None created_by=None updated_at=None updated_by=None object_permission_id=None object_permission=None rotation_count=0 auto_rotate=False rotation_interval=None last_rotation_at=None key_rotation_at=None team_spend=None team_alias=None team_tpm_limit=None team_rpm_limit=None team_max_budget=None team_models=[] team_blocked=False soft_budget=None team_model_aliases=None team_member=None team_metadata=None team_object_permission_id=None team_member_spend=None team_member_tpm_limit=None team_member_rpm_limit=None end_user_id=None end_user_tpm_limit=None end_user_rpm_limit=None end_user_max_budget=None organization_max_budget=None organization_tpm_limit=None organization_rpm_limit=None organization_metadata=None last_refreshed_at=1770297794.589821 api_key='054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d' user_role=<LitellmUserRoles.PROXY_ADMIN: 'proxy_admin'> allowed_model_region=None parent_otel_span=None rpm_limit_per_model=None tpm_limit_per_model=None user_tpm_limit=None user_rpm_limit=None user_email=None request_route='/v1/chat/completions'
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7e61e309de50> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7e61e3f79d30> is disabled via headers. Disable callbacks from headers: None
aether-gateway | INFO:     172.18.0.1:47332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: parallel_request_limiter_v3.py:1342 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: parallel_request_limiter_v3.py:1256 - Executing TTL-preserving increment for key={api_key:054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d}:max_parallel_requests, increment=-1, ttl=60
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: parallel_request_limiter_v3.py:1256 - Executing TTL-preserving increment for key={api_key:054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d}:tokens, increment=256, ttl=60
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: parallel_request_limiter_v3.py:1256 - Executing TTL-preserving increment for key={user:default_user_id}:tokens, increment=256, ttl=60
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: parallel_request_limiter_v3.py:1256 - Executing TTL-preserving increment for key={model_per_key:054c5490949c0770cca202638905f7329e529ff4ccb2ce6d492ae7f66321258d:kimi-vl-thinking}:tokens, increment=256, ttl=60
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'kimi-vl-thinking', 'combined_model_name': 'openai/kimi-vl-thinking', 'stripped_model_name': 'kimi-vl-thinking', 'combined_stripped_model_name': 'openai/kimi-vl-thinking', 'custom_llm_provider': 'openai'}
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:4928 - model=kimi-vl-thinking, custom_llm_provider=openai has no input_cost_per_token in model_cost_map. Defaulting to 0.
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:4940 - model=kimi-vl-thinking, custom_llm_provider=openai has no output_cost_per_token in model_cost_map. Defaulting to 0.
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:5171 - model_info: {'key': 'openai/kimi-vl-thinking', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
aether-gateway | 13:23:14 - LiteLLM:DEBUG: litellm_logging.py:1830 - Logging Details LiteLLM-Success Call streaming complete
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:4830 - checking potential_model_names in litellm.model_cost: {'split_model': 'kimi-vl-thinking', 'combined_model_name': 'openai/kimi-vl-thinking', 'stripped_model_name': 'kimi-vl-thinking', 'combined_stripped_model_name': 'openai/kimi-vl-thinking', 'custom_llm_provider': 'openai'}
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:4928 - model=kimi-vl-thinking, custom_llm_provider=openai has no input_cost_per_token in model_cost_map. Defaulting to 0.
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:4940 - model=kimi-vl-thinking, custom_llm_provider=openai has no output_cost_per_token in model_cost_map. Defaulting to 0.
aether-gateway | 13:23:14 - LiteLLM:DEBUG: utils.py:5171 - model_info: {'key': 'openai/kimi-vl-thinking', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'openai', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if cache is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <bound method Router.sync_deployment_callback_on_success of <litellm.router.Router object at 0x7e61e406cd70>> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7e61e5030440> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.router_strategy.least_busy.LeastBusyLoggingHandler object at 0x7e61e406d400> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.proxy_track_cost_callback._ProxyDBLogger object at 0x7e61e3f7be00> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7e61e309de50> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter_v3._PROXY_MaxParallelRequestsHandler_v3 object at 0x7e61e3f79d30> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7e61e309e490> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7e61e3f79fd0> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7e61e3f7a120> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7e61e509d350> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM Proxy:DEBUG: parallel_request_limiter_v3.py:1294 - Successfully executed TTL-preserving increment for 4 keys
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7e61e309e490> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm.proxy.hooks.responses_id_security.ResponsesIDSecurity object at 0x7e61e3f79fd0> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:38 - Checking if <litellm_enterprise.proxy.hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7e61e3f7a120> is disabled via headers. Disable callbacks from headers: None
aether-gateway | 13:23:14 - LiteLLM:DEBUG: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
