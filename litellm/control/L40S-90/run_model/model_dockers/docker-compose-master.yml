# Each Comment Line shows breaks for individual docker-compose set ups
version: "3.9"
# 2 Models on this card L40S-90
networks:
  aether-ops:
    external: true

services:

  minicpm-v:
    image: vllm/vllm-openai:latest
    container_name: minicpm-v
    networks:
      - aether-ops
    extra_hosts:
      - host.docker.internal:host-gateway
    env_file:
      - .env
    runtime: nvidia

    environment:
      - HF_HOME=/cache
      - TRANSFORMERS_CACHE=/cache
      - VLLM_VIDEO_LOADER_BACKEND=enhanced_opencv
    volumes:
      - /mnt/aetherpro/models:/models
      - /mnt/aetherpro/cache:/cache

    ports:
      - "8101:8000"

    command: >
      --model /models/vision/openbmb/MiniCPM-V-4_5-AWQ
      --served-model-name minicpm-v-4.5
      --dtype auto
      --gpu-memory-utilization 0.35
      --max-model-len 4096
      --trust-remote-code
      --port 8000
      --enable-auto-tool-choice
      --tool-call-parser openai
      --chat-template content_format=openai
      --disable-log-requests
      --enable-prefix-caching
      --limit-mm-per-prompt image=16,video=4
      --api-key EMPTY
      --generation-config vllm
  

    restart: unless-stopped


  qwen3-30b-thinking:
    image: vllm/vllm-openai:latest
    container_name: qwen3-30b-thinking
    networks:
      - aether-ops
    env_file:
      - .env
    runtime: nvidia

    environment:
      - HF_HOME=/cache
      - TRANSFORMERS_CACHE=/cache

    volumes:
      - /mnt/aetherpro/models:/models
      - /mnt/aetherpro/cache:/cache

    ports:
      - "8102:8000"

    command: >
      --model /models/llm/cyankiwi/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit
      --served-model-name qwen3-30b-thinking
      --dtype auto
      --gpu-memory-utilization 0.65
      --max-model-len 8192
      --port 8000
      --enable-auto-tool-choice 
      --tool-call-parser openai 
      --enable-prefix-caching 
      --chat-template content_format=openai
      --disable-log-requests
      --generation-config vllm


    restart: unless-stopped

# 1 Model on this card L40S-90
  qwen3-vl-thinking:
    image: vllm/vllm-openai:latest
    container_name: qwen3-vl-thinking
    networks:
      - aether-ops
    env_file:
      - .env
    runtime: nvidia

    environment:
      - HF_HOME=/cache
      - TRANSFORMERS_CACHE=/cache

    volumes:
      - /mnt/aetherpro/models:/models
      - /mnt/aetherpro/cache:/cache

    ports:
      - "8103:8000"

    command: >
      --model /models/llm/cyankiwi/Qwen3-VL-30B-Instruct-AWQ-4bit
      --served-model-name qwen3-vl-thinking
      --dtype auto
      --gpu-memory-utilization 0.95
      --max-model-len 131072 \
      --enable-auto-tool-choice \
      --tool-call-parser openai \
      --enable-prefix-caching \
      --chat-template content_format=openai

    restart: unless-stopped

# 1 Model on this dual card server L40S-180

networks:
  aether-ai:
    external: true

services:
  qwen3-next-80b:
    image: vllm/vllm-openai:latest
    container_name: qwen3-next-instruct

    runtime: nvidia

    networks:
      - aether-ai
    extra_hosts:
      - "host.docker.internal:host-gateway"

    env_file:
      - .env

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - HF_HOME=/models

    volumes:
      # CHANGE THIS IF YOUR PATH IS SLIGHTLY DIFFERENT
      - /mnt/aetherpro-extra1/llms/Qwen3/cyankiwi/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit:/models
      - ./logs/qwen3-next:/logs

    command:
      --model /models
      --served-model-name qwen3-next-instruct
      --host 0.0.0.0
      --port 8001
      --tensor-parallel-size 2
      --gpu-memory-utilization 0.92
      --max-model-len 32768
      --dtype auto
      --disable-log-requests
      --max-num-seqs 8
      --max-num-batched-tokens 8192
      --swap-space 8
      --enable-auto-tool-choice
      --tool-call-parser hermes
      --enable-prefix-caching
      --disable-custom-all-reduce
      --generation-config vllm
    restart: unless-stopped