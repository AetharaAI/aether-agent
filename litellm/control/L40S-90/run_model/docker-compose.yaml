networks:
  aether-ops:
    external: true

services:

  minicpm-v:
    image: vllm/vllm-openai:latest
    container_name: minicpm-v
    networks:
      - aether-ops
    extra_hosts:
      - host.docker.internal:host-gateway
    env_file:
      - .env
    runtime: nvidia

    environment:
      - HF_HOME=/cache
      - VLLM_VIDEO_LOADER_BACKEND=enhanced_opencv
    volumes:
      - /mnt/aetherpro/models:/models
      - /mnt/aetherpro/cache:/cache

    ports:
      - "8101:8000"

    command: >
      --model /models/vision/openbmb/MiniCPM-V-4_5-AWQ
      --served-model-name minicpm-v-4.5
      --dtype auto
      --gpu-memory-utilization 0.35
      --max-model-len 8192
      --trust-remote-code
      --port 8000
      --enable-auto-tool-choice
      --tool-call-parser openai
      --enable-prefix-caching
      --limit-mm-per-prompt {image=2,video=1}
      --api-key EMPTY
      --generation-config vllm
  

    restart: unless-stopped

  nanbeige4-3b-thinking:
    image: vllm/vllm-openai:latest
    container_name: nanbeige4-3b-thinking
    networks:
      - aether-ops
    extra_hosts:
      - host.docker.internal:host-gateway
    env_file:
      - .env
    runtime: nvidia

    environment:
      - HF_HOME=/cache
    volumes:
      - /mnt/aetherpro/models:/models
      - /mnt/aetherpro/cache:/cache

    ports:
      - "8102:8000"

    command: >
      --model /models/llm/Nanbeige/Nanbeige4-3B-Thinking-AWQ-4bit
      --served-model-name nanbeige4-3b-thinking
      --dtype auto
      --gpu-memory-utilization 0.65
      --max-model-len 32768
      --port 8000
      --enable-auto-tool-choice
      --tool-call-parser openai
      --enable-prefix-caching
      --generation-config vllm

    restart: unless-stopped
